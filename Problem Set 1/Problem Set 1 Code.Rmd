---
title: "Problem Set 1 Code"
output: pdf_document
date: "2025-11-13"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo    = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.height = 4     # HÃ¶he in Inches
)
```

# Task 1

In the following a descriptive analysis of the "detailed_fish_market_data", regarding Whiting is conducted. At first the required packages are loaded.

```{r}
## Load packages and dataset
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lmtest)

detailed_data <- read_tsv("../data/detailed_fish_market_data.txt")
```

## Data preparation

Bla Bla Datensatz ec.

```{r, echo = TRUE}
# delete those rows that have NA for 
# "price","quan", "totr", "tots" and 
# filter for whiting (no king)
detailed_whiting <- detailed_data %>%
  filter(!is.na(pric),
         !is.na(quan),
         !is.na(totr),
         type == "w") %>%
  arrange(date)
```

The next code snippet is not shown in the pdf as those are only technical changes. As the dataset has some observations, that are "NA" or relatively obvious outliers, those are removed first. Especially the two obersvations, that seem to stem from another dealer are removed.

```{r, echo=FALSE}

## There seem to be two entries in the dataset, where there are two dealer per day.
# Since this is the case only for two out of all days in April and May: drop those two observations 
detailed_whiting <- detailed_whiting %>%
  # frequency of the same tots value for different days
  group_by(date, dayw, tots) %>%
  mutate(n_same_tots = n()) %>%
  
  # number of distinct tots days
  group_by(date, dayw) %>%
  mutate(n_tots_values = n_distinct(tots)) %>%
  ungroup() %>%
  
  # delete rows for which (there are multiple different tots values 
  #                                   AND 
  #                       for which the tot value only appears once)
  filter(!(n_tots_values > 1 & n_same_tots == 1)) %>%
  
  # delete rows that are not longer needed
  select(-n_same_tots, -n_tots_values)


## two cases, where > 1 dealer is present
tots_inconsistent <- detailed_whiting |>
  group_by(date, dayw) |>
  mutate(
    n_tots = n_distinct(tots)
  ) |>
  filter(n_tots > 1) |>
  arrange(date, dayw, tots, totr)




## define variables for the plots
# themes for plots
theme_fontsize <- theme(
  plot.title = element_text(size = 13),
  plot.subtitle = element_text(size = 9),
  axis.title = element_text(size = 11),
  axis.text = element_text(size = 10),
  legend.text = element_text(size = 10),
)

theme_fontsize_large <- theme(
  plot.title = element_text(size = 16),
  plot.subtitle = element_text(size = 12),
  axis.title = element_text(size = 14),
  axis.text = element_text(size = 13),
  legend.text = element_text(size = 13),
)


# labels for time series data plots
date_seq <- seq(
  from = as.Date("1992-04-06"),
  to   = as.Date("1992-05-15"),
  by   = "day"
)

# format as "MM-YYYY"
day_labels <- format(date_seq, "%d-%m")

# Named character vector: names are month_ids
day_lookup_vec <- setNames(day_labels, c(seq(406,430, by = 1),seq(501,515, by=1)))

break_vec_x_axis <- c(seq(406,430, by = 7),seq(504,515, by=7))
all_days_x_axis <- c(seq(406,430, by = 1),seq(501,515, by=1))
```

Dataset for the analysis on daily level is created.

```{r, echo = TRUE}

# dataset for the daily-level
detailed_whiting_daily <- detailed_whiting %>%
  group_by(date) %>%
  summarise(
    avg_pric = mean(pric),
    totr = first(totr),
    tots = first(tots),
    dayw = first(dayw),
    n_trsact = n(),
    strate = first(tots)/first(totr)
  )


```

## Descriptive analysis

```{r}
####
# summary of the daily datset
####
detailed_whiting_daily %>%
  select(totr, tots, n_trsact) %>%
  summary()
```

As the summary indicates, total sales and therefore the total received amount of Whiting in lbs inherit a large amount of variation. The amount of transaction per day also shows a broad variety of values, with the minimum of four and a maximum of 57.

To gain a first insight in the properties of the total sales of Whiting in the period of April to May 1992 a bar-chart and a time-series plot are used.

```{r, echo = FALSE}
####
# barchart average sales by dayw (Day of the Week)
####
plot_average_sales_by_weekday <- detailed_whiting_daily %>%
  group_by(dayw) %>%
  summarise(avg_tots = mean(tots, na.rm = TRUE)) %>%
  ggplot(aes(x = factor(dayw), y = avg_tots)) +
  geom_col(fill = "lightblue", colour = "grey20", width = 0.4) + 
  labs(title = "Average Total Sales by Weekday",
       subtitle = "Whiting sales, April-May 1992",
       y = "Sales (lbs)",
       x = NULL)+
  geom_text(
    aes(label = scales::comma(round(avg_tots, 0))),
    vjust = -0.3,
    size = 3
  ) +
  scale_x_discrete(breaks = 1:5,
                   labels = c("Mon", "Tue", "Wed", "Thu", "Fri")) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large


####
# time series of tots (total sales)
####
tots_plot_df <- detailed_whiting_daily %>%
  select(date, tots) %>%
  complete(date = 406:515,
           fill = list(tots = 0)) %>%
  arrange(date) %>%
  filter(!between(date, 431, 500)) %>%
  mutate(date_fac = factor(date, levels = date))

plot_daily_sales_over_time <- ggplot(tots_plot_df, aes(x=date_fac, y = tots, group = 1)) + 
  geom_col(width = 0.2,
           colour = "lightblue",
           fill="grey10") +
  labs(title = "Daily Total Sales over Time",
       subtitle = "Whiting sales, April-May 1992",
       y = "Sales (lbs)",
       x = NULL)+ 
  scale_x_discrete(breaks = as.character(all_days_x_axis),
                   labels = function(x) {
                     lab <- rep("", length(x))
                     sel <- x %in% as.character(break_vec_x_axis)
                     lab[sel] <- day_lookup_vec[x[sel]]
                     lab
                   }) +
  theme_bw() +
  theme_fontsize_large

```

```{r ,fig.show = "hold", out.width = "50%"}
plot_average_sales_by_weekday
plot_daily_sales_over_time
```

Plot Beschreibung.

```{r, echo = FALSE}
####
# time series of str (sell trough rate)
####
str_plot_df <- detailed_whiting_daily %>%
  select(date, strate) %>%
  complete(date = 406:515,
           fill = list(strate = 0)) %>%
  arrange(date) %>%
  filter(!between(date, 431, 500)) %>%
  mutate(date_fac = factor(date, levels = date))


plot_daily_str_over_time <- ggplot(str_plot_df, aes(x=date_fac, y = strate, group = 1)) + 
  geom_line(size = 0.5,
            colour = "grey20") +
  geom_point(
    data   = subset(str_plot_df, strate > 0),
    size   = 1,
    colour = "grey10"
  ) +
  geom_abline(intercept = 1, 
              slope = 0, 
              linetype = "dashed",
              size = 0.8,
              colour = "skyblue") + 
  labs(title = "Daily Sell-Through Rate over Time",
       subtitle = "Whiting sales, April-May 1992",
       y = "STR",
       x = NULL)+
  scale_y_sqrt(breaks = c(0, 0.5, 1, 2.5, 5, 10)) + 
  scale_x_discrete(breaks = as.character(all_days_x_axis),
                   labels = function(x) {
                     lab <- rep("", length(x))
                     sel <- x %in% as.character(break_vec_x_axis)
                     lab[sel] <- day_lookup_vec[x[sel]]
                     lab
                   }) +
  theme_bw() +
  theme_fontsize
```

```{r, fig.show = "hold", fig.height= 2.5, out.width = "100%"}
plot_daily_str_over_time
```

Plot Beschreibung.

```{r}
####
# correlation between tots (total dailiy sales) and avg_pric (average price)
####
cor(detailed_whiting_daily$avg_pric,detailed_whiting_daily$tots)
```

Korrelation Beschreibung.

```{r, echo = FALSE}
####
# boxplot pric(price) by dayw (Day of the Week)
####
plot_price_distr_by_weekday <- ggplot(detailed_whiting, 
       aes(x=factor(dayw), y=pric)) +
  geom_boxplot(fill = "lightblue",         # dezente Farbe
               colour = "grey20",
               width = 0.6,
               outlier.colour = "firebrick",
               outlier.alpha = 0.7,
               outlier.size = 2) + 
  labs(
    x = NULL,
    y = "Price (per lbs)",
    title = "Price Distribution by Weekday",
    subtitle = "Whiting sales, April-May 1992"
  ) +
  scale_x_discrete(breaks = seq(1,5,by=1),
                   labels = c("Mon", "Tue", "Wed", "Thu", "Fri")) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large


####
# boxplot pric(price) by qual (Qualitiy of the fish)
####
plot_price_distr_by_quality <- detailed_whiting %>%
  filter(!is.na(qual)) %>%
  ggplot(aes(x=factor(qual), y=pric)) +
  geom_boxplot(fill = "lightblue",
               colour = "grey20",
               width = 0.6,
               outlier.colour = "firebrick",
               outlier.alpha = 0.7,
               outlier.size = 2) + 
  labs(
    x = NULL,
    y = "Price (per lbs)",
    title = "Price Distribution by Whiting Quality",
    subtitle = "Whiting sales, April-May 1992"
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large
```

```{r, fig.show = "hold", out.width = "50%"}
plot_price_distr_by_weekday
plot_price_distr_by_quality
```

Plot Beschreibung.

```{r, echo = FALSE}
hourly_detailed_whiting <- detailed_whiting %>%
  mutate(time = round(time/100, digits = 0))

# number of transactions per hour 
plot_transactions_per_hour <- hourly_detailed_whiting %>%
  group_by(time) %>%
  summarise(n_sales = n()) %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = factor(time), y = n_sales)) +
  geom_col(fill = "lightblue", colour = "grey20", width = 0.4) + 
  labs(title = "Number of Transactions per Hour",
       subtitle = "Whiting sales, April-May 1992",
       y = "Transactions",
       x = NULL)+
  geom_text(
    aes(label = scales::comma(round(n_sales, 0))),
    vjust = -0.3,
    size = 3
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large
  

# average price across hours
plot_average_price_per_hour <- hourly_detailed_whiting %>%
  group_by(time) %>%
  summarise(avg_pric = mean(pric, na.rm = TRUE)) %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = factor(time), y = avg_pric)) +
  geom_col(fill = "lightblue", colour = "grey20", width = 0.4) + 
  labs(title = "Average Price per Hour",
       subtitle = "Whiting sales, April-May 1992",
       y = "Price (per lbs)",
       x = NULL)+
  geom_text(
    aes(label = scales::comma(round(avg_pric, 2))),
    vjust = -0.3,
    size = 3
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large
```

```{r, fig.show = "hold", out.width = "50%"}
plot_transactions_per_hour
plot_average_price_per_hour
```

Plot Beschreibung.

# Task 3: Moderated Regression

### 3.1 Hypotheses

In this task, we investigate whether the price sensitivity of individual customers depends on specific contextual characteristics of the transaction. Based on plausibility considerations, the following three hypotheses were developed:

------------------------------------------------------------------------

**Hypothesis 1 (H1): Moderation by Product Quality (`qual`)**

-   **Hypothesis:** The quality of the fish has an influence on the price sensitivity of customers.
-   **Rationale:** We expect that higher quality (`qual`) leads to *lower* price sensitivity. A high-quality product, which might be sold to expensive restaurants with higher margins, justifies a higher price and makes customers less susceptible to price fluctuations.

**Hypothesis 2 (H2): Moderation by Payment Method (`cash`)**

-   **Hypothesis:** The use of cash versus charge (invoice) influences price sensitivity.
-   **Rationale:** Paying with cash may have a higher "emotional value" (or "pain of paying") as the amount paid is immediately visible, rather than just appearing on a bill later. We, therefore, expect that cash transactions lead to *higher* price sensitivity.

**Hypothesis 3 (H3): Moderation by Establishment Type (`estb`)**

-   **Hypothesis:** Price sensitivity depends on the customer's type of establishment.
-   **Rationale:** "Fry shops" (`f`) likely operate on lower margins for their final products and thus have a stronger incentive to watch purchase prices than "Stores" (`s`). We, therefore, expect that "fry shops" will exhibit *higher* price sensitivity.

```{r}
#1 data preperation
detailed_data_prep <- detailed_data %>%
  filter(!is.na(pric),
         !is.na(quan),
         type == "w") %>%
  arrange(date) %>%
  
  # 1.1 standardization
  group_by(cusn) %>%
  mutate(Qty_Dev = quan - mean(quan, na.rm = TRUE)) %>% #target variable
  ungroup() %>%
  
  # 1.2 mean centering and dummy creation
  mutate(
    price_c = as.numeric(scale(pric, center = TRUE, scale = FALSE)), #main regressor
    quality_c = as.numeric(scale(qual, center = TRUE, scale = FALSE)), # Moderator 1
    cash_dummy = if_else(cash == 'c', 1, 0), # Moderator 2
    estb = as.factor(estb), # Moderator 3
  ) 
```

-   **`filter(...)`**: This is the first cleaning step.

    -   **`!is.na(pric), !is.na(quan)`**: We remove any rows where either price (`pric`) or quantity (`quan`) are missing. We cannot model a price-demand relationship without a price or a quantity, so these rows are unusable for our model.
    -   **`type == "w"`**: We filter the dataset to only include "Whiting". This ensures our analysis is focused on a single product, as combining different fish types would introduce confounding variables.

-   **`arrange(date)`**: This step sorts the resulting data by date. While not strictly required for this regression, it's good practice to organize time-series data chronologically, which can help in identifying patterns or debugging later.

-   **`group_by(cusn)`**: This crucial step groups the data by customer number (`cusn`). It doesn't change the data itself but tells the following `mutate` function to perform its calculations *within* each customer's group.

-   **`mutate(Qty_Dev = ...)`**: This creates our new target (dependent) variable, `Qty_Dev`.

    -   **Justification**: The problem set notes that customers are different sizes. Simply modeling `quan` (quantity) would be misleading, as a large restaurant will always buy more than a small shop, regardless of price.
    -   **Action**: By calculating `quan - mean(quan, na.rm = TRUE)`, we create a **within-customer standardized variable**. `Qty_Dev` now represents how much *more* or *less* a customer bought on a specific day compared to their *own* average. This isolates their behavioral deviation and is a much more accurate variable for modeling price sensitivity.

-   **`ungroup()`**: This step removes the grouping. It's essential "housekeeping" to ensure that the next `mutate` call performs its calculations (like mean-centering) on the *entire* dataset, not on a per-customer basis.

-   **`mutate(...)`**: This final step creates all the predictor variables (regressors) we need for our models.

    -   **`price_c = ...`**: This **mean-centers** the `pric` variable, as recommended in the lecture. Mean-centering (`center = TRUE`, `scale = FALSE`) subtracts the overall average price from each transaction's price. This makes the coefficients in our moderated regression models much easier to interpret. Specifically, the main effect of price will now represent the price sensitivity at the *average* level of the moderator.
    -   **`quality_c = ...`**: This likewise mean-centers our first moderator, `qual` (quality).
    -   **`cash_dummy = ...`**: This converts the categorical `cash` variable into a numeric **dummy variable** for our second hypothesis. The model can interpret "1" (for cash) and "0" (for non-cash), but it cannot interpret the original 'c' and 'h' letters.
    -   **`estb = as.factor(estb)`**: This converts the establishment type (`estb`) variable into a **factor**. This tells R that `estb` is a categorical variable. When we include it in the `lm()` function, R will automatically create the necessary dummy variables for each establishment type, allowing us to test our third hypothesis.

```{r}
# additional steps for establishment
detailed_data_prep %>% group_by(estb) %>% count()

detailed_data_perep_estb = detailed_data_prep %>%
  filter(estb %in% c("s", "f", "sf"))
```

This code block performs a crucial diagnostic check and a subsequent filtering action specifically to prepare for testing Hypothesis 3 (moderation by establishment type).

-   **`detailed_data_prep %>% group_by(estb) %>% count()`**

    -   **What it does**: This line is a **diagnostic check**. It groups the prepared data by the establishment type (`estb`) and counts the number of observations (transactions) for each type.
    -   **Why it's done**: Before using a categorical variable as a moderator, we must check its distribution. The output of this count (seen in the previous step) reveals that while some categories like 's' (store) and 'f' (fry shop) have many observations, other categories have very few (e.g., only 1, 2, or 3).

-   **`detailed_data_perep_estb = ... filter(estb %in% c("s", "f", "sf"))`**

    -   **What it does**: This line **creates a new, filtered dataset** named `detailed_data_perep_estb`. It includes only the rows where the establishment type is one of the three most common: "s", "f", or "sf".
    -   **Why it's done**: This is a **critical step for statistical stability**. Attempting to run a regression or moderation analysis on a categorical level with only 1 or 2 observations is statistically unreliable; the model cannot produce a stable estimate for such a small group. It can lead to model errors or highly misleading results. By filtering down to the well-represented groups, we ensure that our analysis for Hypothesis 3 is robust and that the results are meaningful. This new dataset will be used *only* for the H3 analysis.

    ```{r}
    #2.1 Moderated Model 1
    quality_model = lm(Qty_Dev ~ price_c + quality_c, data = detailed_data_prep)
    quality_model_moderation = lm(Qty_Dev ~ price_c*quality_c, data = detailed_data_prep)
    summary(quality_model)
    summary(quality_model_moderation)

    anova(quality_model, quality_model_moderation)
    lrtest(quality_model, quality_model_moderation)
    ```

```{r}
#2.2 Moderated Model 2
cash_model = lm(Qty_Dev ~ price_c+cash_dummy, data=detailed_data_prep)
cash_model_moderation = lm(Qty_Dev ~ price_c*cash_dummy, data=detailed_data_prep)
summary(cash_model)
summary(cash_model_moderation)

anova(cash_model, cash_model_moderation)
lrtest(cash_model, cash_model_moderation)
```

```{r}
#2.3 Moderated Model 3
estb_model = lm(Qty_Dev ~ price_c + estb, data = detailed_data_perep_estb)
estb_model_moderation = lm(Qty_Dev ~ price_c * estb, data = detailed_data_perep_estb)
summary(estb_model)
summary(estb_model_moderation)

anova(estb_model, estb_model_moderation)
lrtest(estb_model, estb_model_moderation)
```
