---
title: "Problem Set I Solution"
author: 
  - "Tobias Bodentien"
  - "Philipp Grunenberg"
  - "Alexander Haas"
  - "Osama Warshaga"
date: "`r format(Sys.Date(), '%d-%m-%Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo    = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.height = 4     # Höhe in Inches
)
```

# Task 1

The subsequent descriptive analysis is conducted on the detailed fish market data regarding whiting. First, the required packages and dataset are loaded.

```{r}
## Load packages and dataset
library(readr)
library(dplyr)
library(ggplot2)
library(tidyr)
library(lmtest)

detailed_data <- read_tsv("../data/detailed_fish_market_data.txt")
```

## Data preparation

The descriptive analysis will focus on price per pound (*pric*), the total quantity received (*totr*) and sold (*tots*) by a dealer in pounds per day in April and May 1992. Therefore, observations for which these variables are *'NA'* are removed via the 'dplyr' package. The dataset has also been filtered for Whiting, as required for Task 1, and arranged by date.

```{r, echo = TRUE}
# delete those rows that have NA for 
# "price","quan", "totr", "tots" and 
# filter for whiting (no king)
detailed_whiting <- detailed_data %>%
  filter(!is.na(pric),
         !is.na(totr),
         !is.na(tots),
         type == "w") %>%
  arrange(date)

print(paste("Obs:", nrow(detailed_whiting),
            "- Vars:", ncol(detailed_whiting)))
```

The cleaned whiting dataset now comprises 478 observations across 17 variables. 

Inspecting the dataset via *'View(detailed_whiting)'* reveals that there are multiple observations for each day in the period from April to May 1992. Each observation depicts a transaction between a customer and the fish dealer. This is particularly important for understanding the next step.

Of the 478 observations, two do not fit. The total quantity that the dealers sold (*tots*) is equal for all rows (i.e. transactions) of the same dealer at a given day. 

What immediately stands out is that only one dealer is observed on almost all days. For this dealer, multiple transactions are always recorded. Only on two days are two dealers observed, as indicated by multiple distinct observations for *tots*. However, these additional dealers are unrepresentative, as there is only one observation for each of them. The remaining observations all appear to come from a single, larger dealer. These two observations are therefore deleted using 'dplyr'.


```{r, echo=TRUE}

## There seem to be two entries in the dataset, where there are two dealer per day.
# Since this is the case only for two out of all days in April and May: 
# drop those two observations 
detailed_whiting <- detailed_whiting %>%
  # frequency of the same tots value for different days
  group_by(date, dayw, tots) %>%
  mutate(n_same_tots = n()) %>%
  
  # number of distinct tots days
  group_by(date, dayw) %>%
  mutate(n_tots_values = n_distinct(tots)) %>%
  ungroup() %>%
  
  # delete rows for which (there are multiple different tots values 
  #                                   AND 
  #                       for which the tot value only appears once)
  filter(!(n_tots_values > 1 & n_same_tots == 1)) %>%
  
  # delete rows that are not longer needed
  select(-n_same_tots, -n_tots_values)

## two cases, where > 1 dealer is present
tots_inconsistent <- detailed_whiting |>
  group_by(date, dayw) |>
  mutate(
    n_tots = n_distinct(tots)
  ) |>
  filter(n_tots > 1) |>
  arrange(date, dayw, tots, totr)
```

With this step the data cleaning process is finished. 

One part of the descriptive analysis of the Whiting data involves analysing data aggregated at a daily level. For this purpose, a new dataset, *detailed_whiting_daily*, is constructed. 

```{r, echo = TRUE}

# dataset for the daily-level
detailed_whiting_daily <- detailed_whiting %>%
  group_by(date) %>%
  summarise(
    avg_pric = mean(pric),
    totr = first(totr),
    tots = first(tots),
    dayw = first(dayw),
    n_trsact = n(),
    strate = first(tots)/first(totr)
  )


```

For each date (*date*) present in the original dataset, the average price (*avg_pric*), *totr*, *tots*, number of transactions (*n_trsact*) and sell-through rate (*strate*) are computed. For any computation involving *totr* or *tots* the first value of the day can be used. This is possible due to the previous data cleaning step, as described above. The sell-through rate measures the proportion of Whiting offered for sale on a given day that is actually sold. More on that later.

## Descriptive analysis

The analysis is split into three sections: Firstly, an analysis of daily sales; secondly, an analysis of prices; and finally, an analysis based on the hours of the day.

In a code chunk that is not printed in this PDF (*'echo=FALSE'*) the different font sizes and some technical variables for the plots are defined. All plots were created using the 'ggplot2' package.

```{r, echo=FALSE}


## define variables for the plots
# themes for plots
theme_fontsize <- theme(
  plot.title = element_text(size = 13),
  plot.subtitle = element_text(size = 9),
  axis.title = element_text(size = 11),
  axis.text = element_text(size = 10),
  legend.text = element_text(size = 10),
)

theme_fontsize_large <- theme(
  plot.title = element_text(size = 16),
  plot.subtitle = element_text(size = 12),
  axis.title = element_text(size = 14),
  axis.text = element_text(size = 13),
  legend.text = element_text(size = 13),
)


# labels for time series data plots
date_seq <- seq(
  from = as.Date("1992-04-06"),
  to   = as.Date("1992-05-15"),
  by   = "day"
)

# format as "MM-YYYY"
day_labels <- format(date_seq, "%d-%m")

# Named character vector: names are month_ids
day_lookup_vec <- setNames(day_labels, c(seq(406,430, by = 1),seq(501,515, by=1)))

break_vec_x_axis <- c(seq(406,430, by = 7),seq(504,515, by=7))
all_days_x_axis <- c(seq(406,430, by = 1),seq(501,515, by=1))
```


```{r}
####
# summary of the daily datset
####
detailed_whiting_daily %>%
  select(totr, tots, n_trsact) %>%
  summary()

detailed_whiting_daily %>%
  select(totr, tots, n_trsact) %>%
  summarise(
    across(everything(), \(x) sd(x, na.rm = TRUE))
  )

```

**Insight 1:** The summary statistics for *totr*, *tots* and *n_trsact* at a daily level show that the central tendencies of *tots* and *totr* are quite similar, as expected.  Furthermore, both exhibit a significant degree of variation, with standard deviations of 4,381 and 3,691, respectively.  The number of daily transactions also varies considerably, ranging from very quiet to very busy days (minimum of four and a maximum of 57). Overall, these findings suggest substantial day-to-day volatility in the Whiting business. 

\newpage

### Sales analysis

```{r, echo = FALSE}
####
# barchart average sales by dayw (Day of the Week)
####
plot_average_sales_by_weekday <- detailed_whiting_daily %>%
  group_by(dayw) %>%
  summarise(avg_tots = mean(tots, na.rm = TRUE)) %>%
  ggplot(aes(x = factor(dayw), y = avg_tots)) +
  geom_col(fill = "lightblue", colour = "grey20", width = 0.4) + 
  labs(title = "Average Total Sales by Weekday",
       subtitle = "Whiting sales, April-May 1992",
       y = "Sales (lbs)",
       x = NULL)+
  geom_text(
    aes(label = scales::comma(round(avg_tots, 0))),
    vjust = -0.3,
    size = 3
  ) +
  scale_x_discrete(breaks = 1:5,
                   labels = c("Mon", "Tue", "Wed", "Thu", "Fri")) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large


####
# time series of tots (total sales)
####
tots_plot_df <- detailed_whiting_daily %>%
  select(date, tots) %>%
  complete(date = 406:515,
           fill = list(tots = 0)) %>%
  arrange(date) %>%
  filter(!between(date, 431, 500)) %>%
  mutate(date_fac = factor(date, levels = date))

plot_daily_sales_over_time <- ggplot(tots_plot_df, aes(x=date_fac, y = tots, group = 1)) + 
  geom_col(width = 0.2,
           colour = "lightblue",
           fill="grey10") +
  labs(title = "Daily Total Sales over Time",
       subtitle = "Whiting sales, April-May 1992",
       y = "Sales (lbs)",
       x = NULL)+ 
  scale_x_discrete(breaks = as.character(all_days_x_axis),
                   labels = function(x) {
                     lab <- rep("", length(x))
                     sel <- x %in% as.character(break_vec_x_axis)
                     lab[sel] <- day_lookup_vec[x[sel]]
                     lab
                   }) +
  theme_bw() +
  theme_fontsize_large

```

```{r ,fig.show = "hold", out.width = "50%"}
plot_average_sales_by_weekday
plot_daily_sales_over_time
```

**Insight 2:** The plot on the left shows the average weekly total sales of whiting by day of the week. Sales are lowest on Wednesdays, increasing towards the end of the week with notably higher average volumes on Thursdays and Fridays. This suggests that demand for whiting is strongest just before the weekend.

The right-hand plot shows the total daily sales over the period April–May 1992. It is immediately clear from this plot that only 19 distinct days are observed throughout this period. As this results in a relatively small number of observations at the level of individual days, a boxplot is not used for the analysis. There is considerable fluctuation in sales from day to day, with several pronounced spikes and some much quieter days. This confirms a high level of volatility in daily whiting sales over the observed period. Another interesting observation is that the market was closed on Monday 20 April 1992. This resulted in unusually high sales the next day compared to other Tuesdays in the sample.

Please note, that the code for all plots of Task 1 can be accessed in the '*.Rmd*' file of our Problem Set 1 solution.

```{r, echo = FALSE}
####
# time series of str (sell trough rate)
####
str_plot_df <- detailed_whiting_daily %>%
  select(date, strate) %>%
  complete(date = 406:515,
           fill = list(strate = 0)) %>%
  arrange(date) %>%
  filter(!between(date, 431, 500)) %>%
  mutate(date_fac = factor(date, levels = date))


plot_daily_str_over_time <- ggplot(str_plot_df, aes(x=date_fac, y = strate, group = 1)) + 
  geom_line(size = 0.5,
            colour = "grey20") +
  geom_point(
    data   = subset(str_plot_df, strate > 0),
    size   = 1,
    colour = "grey10"
  ) +
  geom_abline(intercept = 1, 
              slope = 0, 
              linetype = "dashed",
              size = 0.8,
              colour = "skyblue") + 
  labs(title = "Daily Sell-Through Rate over Time",
       subtitle = "Whiting sales, April-May 1992",
       y = "STR",
       x = NULL)+
  scale_y_sqrt(breaks = c(0, 0.5, 1, 2.5, 5, 10)) + 
  scale_x_discrete(breaks = as.character(all_days_x_axis),
                   labels = function(x) {
                     lab <- rep("", length(x))
                     sel <- x %in% as.character(break_vec_x_axis)
                     lab[sel] <- day_lookup_vec[x[sel]]
                     lab
                   }) +
  theme_bw() +
  theme_fontsize
```

```{r, fig.show = "hold", fig.height= 2.5, out.width = "100%"}
plot_daily_str_over_time
```

**Insight 3:** This line chart shows the daily sell-through rate per day over time. It is calculated as the ratio of *tots* to *totr* (i.e. *tots* divided by *totr*). The dashed line at one indicates full sell-through of the day’s deliveries in Whiting. 

On most trading days, the sell-through rate fluctuates around this value, suggesting that sales and incoming supply are roughly balanced. However, there are a few extreme spikes, especially in mid-April and early May, where the sell-through rate is far above one. These indicate days on which much more was sold than was delivered, meaning existing inventory must have been used up,  as can be seen in the previous days spikes (below one). Overall, the figure indicates highly volatile, yet fairly efficient, inventory usage. This is possible because the Whiting can be frozen and sold over the next few days.

### Price analysis

```{r}
####
# correlation between tots (total daily sales) and avg_pric (average price)
####
cor(detailed_whiting_daily$avg_pric,detailed_whiting_daily$tots)
```

With a correlation of around –0.42, there is a moderate negative linear relationship between average daily prices (*avg_pric*) and total daily sales (*tots*). On days with higher prices, total whiting sales tend to be lower.

```{r, echo = FALSE}
####
# boxplot pric(price) by dayw (Day of the Week)
####
plot_price_distr_by_weekday <- ggplot(detailed_whiting, 
       aes(x=factor(dayw), y=pric)) +
  geom_boxplot(fill = "lightblue",         # dezente Farbe
               colour = "grey20",
               width = 0.6,
               outlier.colour = "firebrick",
               outlier.alpha = 0.7,
               outlier.size = 2) + 
  labs(
    x = NULL,
    y = "Price (per lbs)",
    title = "Price Distribution by Weekday",
    subtitle = "Whiting sales, April-May 1992"
  ) +
  scale_x_discrete(breaks = seq(1,5,by=1),
                   labels = c("Mon", "Tue", "Wed", "Thu", "Fri")) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large


####
# boxplot pric(price) by qual (Qualitiy of the fish)
####
plot_price_distr_by_quality <- detailed_whiting %>%
  filter(!is.na(qual)) %>%
  ggplot(aes(x=factor(qual), y=pric)) +
  geom_boxplot(fill = "lightblue",
               colour = "grey20",
               width = 0.6,
               outlier.colour = "firebrick",
               outlier.alpha = 0.7,
               outlier.size = 2) + 
  labs(
    x = NULL,
    y = "Price (per lbs)",
    title = "Price Distribution by Whiting Quality",
    subtitle = "Whiting sales, April-May 1992"
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large
```

```{r, fig.show = "hold", out.width = "50%"}
plot_price_distr_by_weekday
plot_price_distr_by_quality
```

**Insight 4:** The boxplots of the price per pound of whiting for different weekdays (left-hand side) and different qualities of whiting (right-hand side) are both calculated at transaction level (i.e. using *detailed_whiting*). 

Prices tend to be lower and less dispersed at the beginning of the week (Monday–Tuesday), becoming higher and more dispersed on Wednesday and especially Thursday. Friday's prices are usually similar to those in the middle of the week, but there are a few very high outliers. This could indicate occasional 'premium' pricing at the end of the week.

The figure on the right shows an unexpected quality effect: higher-quality grades of whiting achieve higher prices, at least in terms of the median. However, the price of grade three is similar to that of grade one and shows substantially higher variability. The maximum price for grade three is also very high compared to grade one. As expected, grades four and five fetch clearly lower prices. This could indicate that customers do not expect Whiting to be a 'premium' product, and that good quality is sufficient.


### Analysis by time of day

```{r, echo = FALSE}
hourly_detailed_whiting <- detailed_whiting %>%
  mutate(time = round(time/100, digits = 0))

# number of transactions per hour 
plot_transactions_per_hour <- hourly_detailed_whiting %>%
  group_by(time) %>%
  summarise(n_sales = n()) %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = factor(time), y = n_sales)) +
  geom_col(fill = "lightblue", colour = "grey20", width = 0.4) + 
  labs(title = "Number of Transactions per Hour",
       subtitle = "Whiting sales, April-May 1992",
       y = "Transactions",
       x = NULL)+
  geom_text(
    aes(label = scales::comma(round(n_sales, 0))),
    vjust = -0.3,
    size = 3
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large
  

# average price across hours
plot_average_price_per_hour <- hourly_detailed_whiting %>%
  group_by(time) %>%
  summarise(avg_pric = mean(pric, na.rm = TRUE)) %>%
  filter(!is.na(time)) %>%
  ggplot(aes(x = factor(time), y = avg_pric)) +
  geom_col(fill = "lightblue", colour = "grey20", width = 0.4) + 
  labs(title = "Average Price per Hour",
       subtitle = "Whiting sales, April-May 1992",
       y = "Price (per lbs)",
       x = NULL)+
  geom_text(
    aes(label = scales::comma(round(avg_pric, 2))),
    vjust = -0.3,
    size = 3
  ) +
  scale_y_continuous(labels = scales::comma) +
  theme_bw() +
  theme_fontsize_large
```

```{r, fig.show = "hold", out.width = "50%"}
plot_transactions_per_hour
plot_average_price_per_hour
```

**Insight 5:** The figures on the number of transactions and average price per hour confirm the typical dynamics of a fish market. The market opens at night and peak activity is observed around 6 a.m., after which the transaction volume declines rapidly. According to the bar plot on the right, prices remain relatively stable during the early morning, but then drop sharply at around 8 a.m., in line with the decline in trading activity.

## Conclusion

Overall, the analyses provide a fairly consistent picture of the Whiting market. Nothing stands out as being different from what would be expected in a non-premium fish market.

# Task 2: Nonlinear Regression

In Exercise 2, we examine the relationship between the quantity of sold fish and its price. First, a linear regression is conducted, and outliers are identified using three different methods: leverage, studentized residuals, and Cook’s distance. 
In the second part of the exercise, nonlinear regressions are performed, using polynomial regression and logarithmic transformation. The results of the different regression models are then compared based on relevant criteria.

```{r}
# For easier understanding of the code name the log values of price and quantity respectively:
library(readr)
library(dplyr)
daily_data <- read_tsv("../data/daily_fish_market_data.txt")
daily_data <- daily_data %>%
  rename(price_log=price, qty_log=qty)
# Now calculate price and quantity.
daily_data$price <- exp(daily_data$price_log)
daily_data$qty <- exp(daily_data$qty_log)
# The values of price and qty are almost identical to pricelevel and tots in most cases. Nevertheless, the calculated price and qty will be used in the following as there are sometimes bigger differences between qty and tots, e.g. in datapoint 24:
daily_data$qty[24] - daily_data$tots[24]
```

## Task 2.1: Outliers

Before identifying the outliers we need to calculate the linear regression of quantity and price:

```{r}
linear_reg = lm(daily_data$qty~ daily_data$price)
```

Looking at the plot we see, that some datapoints might influence the regression strongly or have a high residual. Outliers can lead to skewed statistical results or in the case of regression to overfitting.

\vspace{-14mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
par(cex = 0.7)
plot(daily_data$price, daily_data$qty, xlab="Price", ylab="Quantity",
     cex  = 0.7)
abline(linear_reg, col = "red", lwd = 1)
```

\vspace{-9mm}

To identify outliers three methods are used.

### Leverage

To measure the influence of an individual observation on the final result the weight w_i of each obersvation i and mean are used. Observations with an higher influence on x have a higher wheigt.
The resulting measure is called leverage and calculates the hatvalues h for each observation.

```{r}
# Compute leverage h_i=1/n+w_i
h = hatvalues(linear_reg)
```

We now compare the highest h values with 2/n, with n being the number of observations. When all observation's hat values are h_i = 2/n, this means that they all influence the regression in the same amount.

```{r, echo=FALSE}
# Get highest h values and compare them to 2/n
top10_h_values <- order(h, decreasing = TRUE)[1:10]
n = nobs(linear_reg)
equal_influence = 2 / n
paste("2/n =", equal_influence)
h[top10_h_values]
```

Observations 5, 111 and 25 deviate from 2/n substantially. This shows that those values have a much higher influence on the regression than other datapoints. Looking at the plot we see that those datapoints are the ones with the highest price and lowest quantity.

\vspace{-5mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
par(cex = 0.7)
plot(daily_data$price, daily_data$qty, xlab="Price", ylab="Quantity", main="linear regression with leverage highlighted",
     cex  = 0.7)
abline(linear_reg, col = "red", lwd = 1)
text(daily_data$price[top10_h_values],
     daily_data$qty[top10_h_values],
     labels = top10_h_values,
     pos = 3)
```

\vspace{-9mm}

### Studentized residuals

The idea of this method is that an outlier lies far away from the regression line and therefore has a high residual.
To identify those outliers the studentized residuals are calulated:

```{r}
r_student <- rstudent(linear_reg)
```

Lets look at the highest and lowest values:

```{r, echo=FALSE}
top10_idx = order(r_student, decreasing = TRUE)[1:10]
low5_idx = order(r_student, decreasing = FALSE)[1:5]
r_student[top10_idx]
r_student[low5_idx]
```

We see that 6 values are above 1.96, which makes them stand out statistically.

\vspace{-5mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
par(cex = 0.7)
top6_idx = order(r_student, decreasing = TRUE)[1:6]
plot(daily_data$price, daily_data$qty, xlab="Price", ylab="Quantity", main="linear regression with studentized residuals highlighted",
     cex  = 0.7)
abline(linear_reg, col = "red", lwd = 1)
text(daily_data$price[top6_idx],
     daily_data$qty[top6_idx],
     labels = top6_idx,
     pos = 2)

```

\vspace{-9mm}

### Cook's distance

The previous two methods either investigated the dependent variable (quantity) in the case of leverage or the independent variable (price) in the case of studentized residuals. Cook's distance combines both methods and considers the dependent and independent variable simultaneously. 

```{r}
# standard deviation of residuals:
sd_res = sd(resid(linear_reg))
# we want to calculate with the previously used residuals and h values
# therefore do not use the fallback of cooks.distance()
D <- cooks.distance(linear_reg, res = r_student, sd= sd_res, hat=h)

```

```{r, echo=FALSE}
top10_D_idx <- order(D, decreasing = TRUE)[1:10]
D[top10_D_idx]
```

Very large D_i values indicate a substantial impact of observation i. In this case the values are low. The highest values are from observations 68 and 54, which also have significantly high studentized residuals. Observation 24 is among the highest values of studentized residuals, hat values and therefore also D values.

\vspace{-14mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
par(cex = 0.7)
plot(daily_data$price, daily_data$qty,
     xlab = "Price", ylab = "Quantity",
     main="linear regression with Cooks distance highlighted",
     cex  = 0.7)
abline(linear_reg, col = "red", lwd = 1)
text(daily_data$price[top10_D_idx],
     daily_data$qty[top10_D_idx],
     labels = top10_D_idx,
     pos = 2, cex = 0.8, col = "blue")
```

\vspace{-9mm}

### Interpretation:

As observed previously, the values with the highest h values are those that achieve the lowest quantity at the highest price. This aligns with the intuition that sales numbers decline as prices rise for general goods. For this reason, we do not assume that these values are outliers. Rather, we assume that they are influential values that occur infrequently yet represent the underlying distribution.

There are six values with statistically significant high studentized residuals. Since the studentized residuals diagnose the dependent variable, the quantity of fish sold stands out as unusually high compared to other observations. A closer look at the highest observations, 68 and 54, shows that not only the quantity sold but also the amount of fish received on those days was particularly high. Accordingly, this is also consistent with the intuition that when larger quantities are available, more is sold. Furthermore, these observations also have a  comparatively low price.

Cook’s distance is a combination of leverage and studentized residuals. Here as well, observations 68 and 54 show the highest values. For the reasons described above, these observations are not considered outliers and remain in the dataset.
Observation 24 stands out across all three methods. This observation corresponds to a stormy Monday with a high amount of fish received. What is notable is the high price despite a large quantity of fish sold. However, this may be justified by the weather and the fact that it is a Monday. Since sales are generally higher on Mondays and restaurants could not purchase fish the day before, they may be more willing to pay a higher price, especially considering that the weather is expected to remain stormy for the following two days, which would likely result in a lower fish supply.

Therefore we do not identify any datapoints as outliers and will not delete any from the dataset.

## Task 2.2: Models for the price-demand relationship

Until now we worked with the linear regression of quantity and price, which assumes a linear relationship between price and quantity. But often, the price-demand function is assumed to be quasi-linear. Therefore, in this task we will use variable transformation and polynomial regression to investigate whether non-linear regression fits better to the data.
We decided to use a log-log model, lin-log model and squared polynomial model. 
The log-log model transforms the dependent and independent variable and therefore assumes the following underlying function:
Quantity = alpha * Price ^ beta, beta < 0. 
The linear-log model only transforms the independent variable.
Both functions can assume a quasi-linear shape and are therefore suitable for this analysis.

Regarding the polynomial regrassion, the following economical hypothesis can be made:
Whiting is a fish that is very cheap compared to other fish. For low prices the demand can be assumed to be high and fall to a zone of indifference for mid-range prices, as the customer still wants fish. When the price is high and similar to other fish the demand falls, as people more likely consume fish of higher quality. 
Therefore, a s-shaped function with a zone of indifference can also be assumed with the following underlying function:
quantity = alpha + beta_1 * x + beta_2 * x^2 + beta_3 * x^3, beta_3 < 0

Looking at the data a log-log and linear-log model can easily be assumed. An s-shaped curve can hardly be seen.

\vspace{-14mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
par(cex = 0.7)
plot(daily_data$price, daily_data$qty, xlab="Price", ylab="Quantity",
     cex  = 0.7)

```

\vspace{-9mm}

Although we do not see a polynomial function, the task requires to perform a polynomial regression.
Therefore, firstly mean centering is conducted:

```{r}
daily_data$price_MC = daily_data$price-mean(daily_data$price, na.rm=TRUE)
```

Then we compute the polynomials.

```{r}
# 2.4.2 Compute polynomials
daily_data$price_MC_squared = daily_data$price_MC * daily_data$price_MC
daily_data$price_MC_cubic = daily_data$price_MC_squared * daily_data$price_MC
daily_data$price_MC_power4 = daily_data$price_MC_cubic * daily_data$price_MC
```

The linear model already explains about 9% of the variance (R-squared = 9,5% and Adjusted R-squared = 8,6%).

```{r}
# running the linear model
summary(linear_reg)

```

The AIC and BIC of the linear regression are:

```{r}
c(AIC(linear_reg), BIC(linear_reg))
```

Lets look at the polynomial regression:

```{r}
# 2.4.3 Analyze and test hypothesized model
polynomial_regression_cubic = lm(daily_data$qty~ daily_data$price_MC+daily_data$price_MC_squared+daily_data$price_MC_cubic)
summary(polynomial_regression_cubic)
c(AIC(polynomial_regression_cubic), BIC(polynomial_regression_cubic))

```

As we now have more independent variables in the polynomial regression the R-squared is slightly higher than in the linear regression. Therefore, we use the adjusted R-squared to evaluate the polynomial regression, which is worse than that of the linear regression. Also AIC and BIC are worse. By visualizing the regression we see, that the assumed s-shape did not take form. The p-values also do not support the hypothesis.

\vspace{-5mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
par(cex = 0.7)
plot(daily_data$price, daily_data$qty, xlab="Price", ylab="Quantity", main="polynomial model on untransformed data",
     cex  = 0.7) 
a <- coef(polynomial_regression_cubic)[1]   # intercept
b1 <- coef(polynomial_regression_cubic)[2]
b2 <- coef(polynomial_regression_cubic)[3]
b3 <- coef(polynomial_regression_cubic)[4]
curve(
  a[1] + x*b1[1] + b2[1]*(x^2) + b3[1]*(x^3),
  from = min(daily_data$price, na.rm = TRUE),
  to   = max(daily_data$price, na.rm = TRUE),
  add  = TRUE,
  col  = "red",
  lwd  = 1
)

```

\vspace{-9mm}

There is no support for the above mentioned hypothesis. 
Therefore we do not analyze the extended model.

Now we investigate the log-log model:
We do not need to transform the data manually, as we already have the log values of price and quantity. We do not have any complications as the minimum of price and quantity is above 0:

```{r}
# the column price and qty or the daily dataset are already log values
min(daily_data$qty) # min > 0
min(daily_data$price) # min > 0
log_log_model = lm(daily_data$qty_log~ daily_data$price_log)
```

The results show, that 7,8% of the variance can be explained, although we cannot compare this value to the linear model as we now work with a transformed dependent variable. The p-value is lower that 5%. AIC and BIC have low values aswell. Indicating, that the log-log explains the data well.

```{r}
summary(log_log_model)
c(AIC(log_log_model), BIC(log_log_model))
```

Looking at the plot of the log-log model on the untransformed data this analysis is supported:

\vspace{-5mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
par(cex = 0.7)
# Koeffizienten aus dem lm holen
b0 <- coef(log_log_model)[1]   # intercept
b1 <- coef(log_log_model)[2]   # first_coeff
plot(daily_data$price, daily_data$qty, xlab="Price", ylab="Quantity", main="log-log model on untransformed data",
     cex  = 0.7) 

curve(
  exp(b0[1]) * x^b1[1],
  from = min(daily_data$price, na.rm = TRUE),
  to   = max(daily_data$price, na.rm = TRUE),
  add  = TRUE,
  col  = "red",
  lwd  = 1
)
```

\vspace{-9mm}

Additionally, we calculate the linear-log model, which compared to the linear regression has better results, as both the R-squared and adjusted R-squared values are higher.

```{r}
# the column price and qty or the daily dataset are already log values
lin_log_model = lm(daily_data$qty~ daily_data$price_log)
summary(lin_log_model)
c(AIC(lin_log_model), BIC(lin_log_model))
```

\vspace{-14mm}

```{r, echo=FALSE, fig.align='center',fig.height=2.5, fig.width=4, out.width='50%'}
# Koeffizienten aus dem lm holen
b0_lin_log <- coef(lin_log_model)[1]   # intercept
b1_lin_log <- coef(lin_log_model)[2]   # first_coeff
par(cex = 0.7)
plot(daily_data$price, daily_data$qty, xlab="Price", ylab="Quantity", main="lin-log model on untransformed data",
     cex  = 0.7) 

curve(
  b0_lin_log[1] + b1_lin_log[1] * log(x),
  from = min(daily_data$price, na.rm = TRUE),
  to   = max(daily_data$price, na.rm = TRUE),
  add  = TRUE,
  col  = "red",
  lwd  = 1
)
```

\vspace{-9mm}

### Conclusion
The polynomial regression did not support the hypothesis of a polynomial function.
We have seen that the linear regression already explains 8.6% of the variance in the model (adjusted R-squared).
When we compare the linear model with the quasi-linear models, we find that the linear-log (Lin-log) model explains 9.1% of the variance. The AIC and BIC values of the two models differ only minimally.
Comparing the log-log model to the Lin-log model, it can be observed that the Lin-log model indicates a higher demand at a lower price than the log-log model. As the price increases, the quantity of fish sold in the Lin-log model decreases more than in the log-log model, eventually converging to a very similar demand level at higher prices. Since the Lin-log model allows better comparability with the linear regression and indicates higher demand at lower prices, we consider the Lin-log model to best describe the data.

# Task 3: Moderated Regression

### 3.1 Hypotheses

In this task, we investigate whether the price sensitivity of individual customers depends on specific contextual characteristics of the transaction. Based on plausibility considerations, the following three hypotheses were developed:

------------------------------------------------------------------------

**Hypothesis 1 (H1): Moderation by Product Quality (`qual`)**

-   **Hypothesis:** The quality of the fish has an influence on the price sensitivity of customers.
-   **Rationale:** We expect that higher quality (`qual`) leads to *lower* price sensitivity. A high-quality product, which might be sold to expensive restaurants with higher margins, justifies a higher price and makes customers less susceptible to price fluctuations.

**Hypothesis 2 (H2): Moderation by Payment Method (`cash`)**

-   **Hypothesis:** The use of cash versus charge (invoice) influences price sensitivity.
-   **Rationale:** Paying with cash may have a higher "emotional value" (or "pain of paying") as the amount paid is immediately visible, rather than just appearing on a bill later. We, therefore, expect that cash transactions lead to *higher* price sensitivity.

**Hypothesis 3 (H3): Moderation by Establishment Type (`estb`)**

-   **Hypothesis:** Price sensitivity depends on the customer's type of establishment.
-   **Rationale:** Due to presumably lower margins, "fry shops" (`f`) have a greater incentive to minimize input costs than "Stores" (`s`). We therefore hypothesize that "fry shops" will display the highest price sensitivity. The mixed category (`sf`) is expected to occupy a middle ground between the two. 

```{r}
#data loading
detailed_data = read_tsv("../data/detailed_fish_market_data.txt")

#1 data preperation
detailed_data_prep <- detailed_data %>%
  filter(!is.na(pric),
         !is.na(quan),
         type == "w") %>%
  arrange(date) %>%
  
  # 1.1 standardization
  group_by(cusn) %>%
  mutate(Qty_Dev = quan - mean(quan, na.rm = TRUE)) %>% #target variable
  ungroup() %>%
  
  # 1.2 mean centering and dummy creation
  mutate(
    price_c = as.numeric(scale(pric, center = TRUE, scale = FALSE)), #main regressor
    quality_c = as.numeric(scale(qual, center = TRUE, scale = FALSE)), # Moderator 1
    cash_dummy = if_else(cash == 'c', 1, 0), # Moderator 2
    estb = as.factor(estb), # Moderator 3
  ) 
```

This is the first cleaning step.

-   We remove any rows where either price (`pric`) or quantity (`quan`) are missing. We cannot model a price-demand relationship without a price or a quantity, so these rows are unusable for our model.
-   We filter the dataset to only include "Whiting". This ensures our analysis is focused on a single product, as combining different fish types would introduce confounding variables.
-   We sort the data ascending by date. While not strictly required for this regression, it's good practice to organize time-series data chronologically, which can help in identifying patterns or debugging later.
-   Next we standardize the consume by customer. This is a crucial step as each customer usually buys different amounts of fish. Therefore we take the mean for every customer and model the amount consumed by the difference to the mean for this customer.
-   Finally we create the main Regressor and the 3 Regressors for the hypothesis.
    -   We mean center `pric` (price). By mean-centering the moderator, the coefficient of price_c in the interaction model Qty_Dev ~ price_c * moderator_c represents price sensitivity at the average level of the moderator. Centering price_c itself mainly affects the interpretation of the intercept and can improve numerical stability, but it is not required for this marginal-effect interpretation.
    -   We likewise mean-center our first moderator, `qual` (quality).
    -   We convert the categorical `cash` variable into a numeric **dummy variable** for our second hypothesis. The model can interpret "1" (for cash) and "0" (for non-cash), but it cannot interpret the original 'c' and '0' letters.
    -   We convert the establishment type (`estb`) variable into a **factor**. This tells R that `estb` is a categorical variable. When we include it in the `lm()` function, R will automatically create the necessary dummy variables for each establishment type, allowing us to test our third hypothesis.

```{r}
# additional steps for establishment
detailed_data_prep %>% group_by(estb) %>% count()

detailed_data_perep_estb = detailed_data_prep %>%
  filter(estb %in% c("s", "f", "sf"))
```

This code block performs a crucial diagnostic check and a subsequent filtering action specifically to prepare for testing Hypothesis 3 (moderation by establishment type).

-   Before using a categorical variable as a moderator, we must check its distribution. The output of this count reveals that while some categories like 's' (store) and 'f' (fry shop) have many observations, other categories have very few (e.g., only 1, 2, or 3).
-    Attempting to run a regression or moderation analysis on a categorical level with only 1 or 2 observations is statistically unreliable; the model cannot produce a stable estimate for such a small group. It can lead to model errors or highly misleading results. By filtering down to the well-represented groups, we ensure that our analysis for Hypothesis 3 is robust and that the results are meaningful. This new dataset will be used *only* for the H3 analysis.

```{r}
#2.1 Moderated Model 1
quality_model = lm(Qty_Dev ~ price_c + quality_c, data = detailed_data_prep)
quality_model_moderation = lm(Qty_Dev ~ price_c*quality_c, data = detailed_data_prep)

summary(quality_model)
summary(quality_model_moderation)
```

The results indicate that in both the basic and the moderated models, the estimated parameters are not statistically significant. The model's explanatory power is extremely low, with an $R^2$ below 1%. Consequently, we fail to reject the null hypothesis; there is no statistical evidence in this dataset that price, quality, or their interaction influences the quantity consumed.

```{r}
#Testing if Moderator 1 is significant
anova(quality_model, quality_model_moderation)
lrtest(quality_model, quality_model_moderation)
```

Neither the ANOVA (F-test for nested models) nor the Likelihood Ratio Test (LRT) indicates a significant difference between the models ($p > 0.05$). This confirms that adding the interaction term does not improve model fit, meaning we find no support for the hypothesis that quality moderates price sensitivity (H1).

```{r}
#2.2 Moderated Model 2
cash_model = lm(Qty_Dev ~ price_c+cash_dummy, data=detailed_data_prep)
cash_model_moderation = lm(Qty_Dev ~ price_c*cash_dummy, data=detailed_data_prep)
summary(cash_model)
summary(cash_model_moderation)
```

Similar to the previous analysis, the parameters in both the main effects and interaction models are statistically insignificant. The adjusted $R^2$ remains negligible (\< 1%). We cannot reject the null hypothesis that the payment method (cash vs. credit) has no impact on consumption behavior or price sensitivity.

```{r}
#Testing if Moderator 2 is significant
anova(cash_model, cash_model_moderation)
lrtest(cash_model, cash_model_moderation)
```

Both the ANOVA and LRT yield non-significant p-values. This suggests that the inclusion of the payment method interaction does not provide a better fit than the simple additive model. Thus, H2 is not supported.

```{r}
#2.3 Moderated Model 3
estb_model = lm(Qty_Dev ~ price_c + estb, data = detailed_data_perep_estb)
estb_model_moderation = lm(Qty_Dev ~ price_c * estb, data = detailed_data_perep_estb)
summary(estb_model)
summary(estb_model_moderation)
```

The analysis of establishment types yields comparable results. None of the coefficients for price, establishment type, or their interaction terms achieve statistical significance. The low $R^2$ values suggest that these variables do not effectively predict deviations in purchase quantity.

```{r}
#Testing if Moderator 3 is significant
anova(estb_model, estb_model_moderation)
lrtest(estb_model, estb_model_moderation)
```

The model comparison tests (ANOVA and LRT) evaluate the collective contribution of the interaction terms associated with the `estb` factor. The results are non-significant, indicating that the relationship between price and demand does not vary significantly across the different establishment types. Therefore, H3 is not supported.
